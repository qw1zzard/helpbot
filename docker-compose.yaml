services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "ollama serve &      
       sleep 10 &&
       ollama pull gemma3:1b
       tail -f /dev/null"
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - backend
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - 6333:6333
    networks:
      - backend
    restart: unless-stopped

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_DB: helpbot
      POSTGRES_USER: helpbot
      POSTGRES_PASSWORD: helpbot
    volumes:
      - pg_data:/var/lib/postgresql/data
    ports:
      - 5432:5432
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "helpbot"]
      interval: 10s
      timeout: 5s
      retries: 5

  helpbot:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: helpbot
    env_file:
      - .env
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data/model:/root/.cache/huggingface
      - ./data/vectorestore:/app/vectorestore
    ports:
      - 80:80
      - 8501:8501
    networks:
      - backend
    depends_on:
      ollama:
        condition: service_started
      qdrant:
        condition: service_started
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  backend:
    driver: bridge

volumes:
  qdrant_data:
  pg_data:
