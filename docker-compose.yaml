services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ./data/ollama:/root/.ollama
    entrypoint: [ "/bin/sh", "-c" ]
    command: >
      "ollama serve &      
       sleep 10 &&
       ollama pull gemma2:2b
       tail -f /dev/null"
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - 11434:11434
    networks:
      - backend
    restart: unless-stopped

  helpbot:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: helpbot
    volumes:
      - ./data/model:/root/.cache/huggingface
      - ./data/vectorestore:/app/vectorestore
    env_file:
      - .env
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - 80:80
      - 8501:8501
    networks:
      - backend
    depends_on:
      - ollama
    restart: unless-stopped

networks:
  backend:
    driver: bridge

volumes:
  ollama:
  model:
  vectorstore:
